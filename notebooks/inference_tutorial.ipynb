{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataJoint Element Facemap: Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open-source data pipeline to automate analyses and organize data\n",
    "\n",
    "In this tutorial, we will walk through processing facial behavior videos using the [Facemap Pose Estimation framework](\"https://github.com/MouseLand/facemap\"). Distinct facial keypoints are determined using a pose estimation model and are stored in the DataJoint pipeline. \n",
    "\n",
    "We will explain the following concepts as they relate to this pipeline:\n",
    "- What is an Element versus a pipeline?\n",
    "- Plot the pipeline with `dj.Diagram`\n",
    "- Insert data into tables\n",
    "- Query table contents\n",
    "- Fetch table contents\n",
    "- Run the pipeline for your experiments\n",
    "\n",
    "For detailed documentation and tutorials on general DataJoint principles that support collaboration, automation, reproducibility, and visualizations:\n",
    "\n",
    "- [DataJoint for Python - Interactive Tutorials](https://github.com/datajoint/datajoint-tutorials) - Fundamentals including table tiers, query operations, fetch operations, automated computations with the `make` function, etc.\n",
    "\n",
    "- [DataJoint for Python - Documentation](https://datajoint.com/docs/core/datajoint-python/)\n",
    "\n",
    "- [DataJoint Element for Facemap - Documentation](https://datajoint.com/docs/elements/element-facemap/)\n",
    "\n",
    "Let's start by importing the packages necessary to run this pipeline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataJoint Element for Facemap Deep Learning\n",
    "\n",
    "#### Open-source data pipeline for processing and analyzing facial behavior videos.\n",
    "\n",
    "Welcome to the tutorial for the DataJoint Element for facial pose estimation and ROI detection. This\n",
    "tutorial aims to provide a comprehensive understanding of the open-source data pipeline\n",
    "created using `element-facemap`.\n",
    "\n",
    "This package is designed to seamlessly process, ingest, and track extracellular electrophysiology\n",
    "data, along with its associated probe and recording metadata. By the end of this\n",
    "tutorial you will have a clear grasp on setting up and integrating `element-facemap`\n",
    "into your specific research projects and lab. \n",
    "\n",
    "![flowchart](../images/diagram_flowchart.svg)\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Please see the [datajoint tutorials GitHub\n",
    "repository](https://github.com/datajoint/datajoint-tutorials/tree/main) before\n",
    "proceeding.\n",
    "\n",
    "A basic understanding of the following DataJoint concepts will be beneficial to your\n",
    "understanding of this tutorial: \n",
    "1. The `Imported` and `Computed` tables types in `datajoint-python`.\n",
    "2. The functionality of the `.populate()` method. \n",
    "\n",
    "#### **Tutorial Overview**\n",
    "\n",
    "+ Setup\n",
    "+ *Activate* the DataJoint pipeline.\n",
    "+ *Insert* subject and session metadata.\n",
    "+ *Populate* Video Recordings and Model information\n",
    "+ Generate and run the pose estimation task.\n",
    "+ Visualize the results.\n",
    "\n",
    "### **Setup**\n",
    "\n",
    "This tutorial examines video data of .mp4 file format. The goal is to store, track\n",
    "and manage sessions of facial pose data, including determining coordinates of facial body parts and\n",
    "trajectory visualizations.\n",
    "\n",
    "The results of this Element can be combined with **other modalities** to create\n",
    "a complete, customizable data pipeline for your specific lab or study. For instance, you\n",
    "can combine `element-facemap` with `element-calcium-imaging` and\n",
    "`element-array-ephys` to relate orofacial behavior to the neural activity.\n",
    "\n",
    "Let's start this tutorial by importing the packages necessary to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the tutorial is run in Codespaces, a private, local database server is created and\n",
    "made available for you. This is where we will insert and store our processed results.\n",
    "Let's connect to the database server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.conn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Activate the DataJoint Pipeline**\n",
    "\n",
    "This tutorial activates the `facemap_inference.py` module from `element-facemap`, along\n",
    "with upstream dependencies from `element-animal` and `element-session`. Please refer to the\n",
    "[`tutorial_pipeline.py`](./tutorial_pipeline.py) for the source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine multiple Elements into a pipeline\n",
    "\n",
    "Each DataJoint Element is a modular set of tables that can be combined into a complete pipeline.\n",
    "\n",
    "Each Element contains 1 or more modules, and each module declares its own schema in the database.\n",
    "\n",
    "This tutorial pipeline is assembled from four DataJoint Elements.\n",
    "\n",
    "The results of this Element example can be combined with other modalities to create a complete customizable data pipeline for your specific lab or study. For instance, you can combine element-facemap with element-calcium-imaging and element-array-ephys to characterize the neural activity.\n",
    "\n",
    "| Element | Source Code | Documentation | Description |\n",
    "| -- | -- | -- | -- |\n",
    "| Element Lab | [Link](https://github.com/datajoint/element-lab) | [Link](https://datajoint.com/docs/elements/element-lab) | Lab management related information, such as Lab, User, Project, Protocol, Source. |\n",
    "| Element Animal | [Link](https://github.com/datajoint/element-animal) | [Link](https://datajoint.com/docs/elements/element-animal) | General animal metadata and surgery information. |\n",
    "| Element Session | [Link](https://github.com/datajoint/element-session) | [Link](https://datajoint.com/docs/elements/element-session) | General information of experimental sessions. |\n",
    "| Element Facemap | [Link](https://github.com/datajoint/element-facemap) | [Link](https://datajoint.com/docs/elements/element-facemap) |  Facemap orofacial bodypart prediction with singular value decomposition or deep learning pose estimation.|\n",
    "\n",
    "By importing the modules for the first time, the schemas and tables will be created in the database.  Once created, importing modules will not create schemas and tables again, but the existing schemas/tables can be accessed.\n",
    "\n",
    "The Elements are imported and activated within the `tutorial_pipeline` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial_pipeline import (\n",
    "    lab,\n",
    "    subject,\n",
    "    session,\n",
    "    fbe,\n",
    "    facemap_inference,\n",
    "    Device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Python module (e.g. subject) contains a schema object that enables interaction with the schema in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python classes in the module correspond to a table in the database server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facemap Inference Schemas Diagram\n",
    "\n",
    "We can represent the `fbe` and `facemap_inference` schemas and their upstream dependencies, `subject` and `session`, using `dj.Diagram()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    dj.Diagram(subject.Subject)\n",
    "    + dj.Diagram(session.Session)\n",
    "    + dj.Diagram(fbe.VideoRecording)\n",
    "    + dj.Diagram(facemap_inference)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the diagram, this data pipeline encompasses tables associated with\n",
    "model and video file data, task generation and results of model inference. A few tables, such as `subject.Subject` or `session.Session`,\n",
    "while important for a complete pipeline, fall outside the scope of the `element-facemap`\n",
    "tutorial, and will therefore, not be explored extensively here. The primary focus of\n",
    "this tutorial will be on the `facemap_inference` schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert entries into `subject` and `session` manual tables\n",
    "\n",
    "Let's start with the first table in the schema diagram (i.e. `subject.Subject` table).\n",
    "\n",
    "To know what data to insert into the table, we can view its dependencies and attributes using the `.describe()` and `.heading` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subject.Subject.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells above show all attributes of the subject table.\n",
    "We will insert data into the\n",
    "`subject.Subject` table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.insert1(\n",
    "    dict(\n",
    "        subject=\"subject1\",\n",
    "        subject_nickname=\"subject1_nickname\",\n",
    "        sex=\"U\",\n",
    "        subject_birth_date=\"2020-01-01\",\n",
    "        subject_description=\"Demo subject for Facemap Pose estimation processing.\",\n",
    "    )\n",
    ")\n",
    "subject.Subject()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the steps above for the `Session` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.Session.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session.heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `describe`, displays the table's structure and highlights its dependencies, such as its reliance on the `Subject` table. These dependencies represent foreign key references, linking data across tables.\n",
    "\n",
    "On the other hand, `heading` provides an exhaustive list of the table's attributes. This\n",
    "list includes both the attributes declared in this table and any inherited from upstream\n",
    "tables.\n",
    "\n",
    "With this understanding, let's move on to insert a session associated with our subject.\n",
    "\n",
    "We will insert into the `session.Session` table by passing a dictionary to the `insert1` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_key = dict(subject=\"subject1\", session_datetime=\"2021-04-30 12:22:15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session.insert1(session_key)\n",
    "session.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every experimental session produces a set of data files. The purpose of the `SessionDirectory` table is to locate these files. It references a directory path relative to a root directory, defined in `dj.config[\\\"custom\\\"]`. More information about `dj.config` is provided in the [documentation](https://datajoint.com/docs/elements/user-guide/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.SessionDirectory.insert1(\n",
    "    dict(**session_key, session_dir=\"subject1/session1\")\n",
    ")\n",
    "session.SessionDirectory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the instantiation of the element-facemap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by inserting a local pytorch model file into the `facemap_inference.FacemapModel` table\n",
    "- Specify a unique `model_id`, a `model_description`, and the `full_local_model_filepath`\n",
    "- The default facemap model is located in the hidden .facemap folder installed to your computer's home directory: `i.e. ~/.facemap/models/facemap_model_state.pt` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'facemap_model_state.pt'\n",
    "model_id = 0\n",
    "model_description = \"test facemap model\"\n",
    "full_local_model_filepath = \"~/.facemap/models/facemap_model_state.pt\"\n",
    "facemap_inference.FacemapModel.insert_new_model(\n",
    "                                                model_name=model_name, \n",
    "                                                model_id=model_id, \n",
    "                                                model_description=model_description, \n",
    "                                                full_model_path=full_local_model_filepath\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the `facemap_inference.FacemapModel` table queried with the model id of interest to verify insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemap_inference.FacemapModel() & f'model_id={model_id}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's display the `facemap_inference.FacemapModel.File` and `facemap_inference.FacemapModel.BodyPart` part tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemap_inference.FacemapModel.File() & f'model_id={model_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemap_inference.FacemapModel.BodyPart() & f'model_id={model_id}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Diagram indicates, `fbe.VideoRecording` table needs to\n",
    "contain data before the `facemap_inference.FacemapPoseEstimationTask` can be generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will insert behavioral video recording data into `fbe.VideoRecording` and its part table, `fbe.VideoRecording.File`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "video_recording_insert = {**session_key, \n",
    "                          \"recording_id\": 0}\n",
    "facemap_root_dir_path = fbe.get_facemap_root_data_dir()\n",
    "vid_path = \"example_behavior_videos mounted S3 inbox path\"\n",
    "video_recording_file_insert = {\n",
    "    **video_recording_insert,\n",
    "    \"file_id\": 0,\n",
    "    \"file_path\": Path(vid_path).relative_to(facemap_root_dir_path),\n",
    "}\n",
    "\n",
    "fbe.VideoRecording.insert1(video_recording_insert)\n",
    "fbe.VideoRecording.File.insert1(video_recording_file_insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an entries present in the `facemap_inference.FacemapModel` and the `fbe.VideoRecording` tables, the criteria is met for insertion into the `facemap_inference.FacemapPoseEstimationTask` table.\n",
    "- `facemap_inference.FacemapPoseEstimationTask` is a staging table that pairs a specific `FacemapModel` with a specific `VideoRecording`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Pose estimation is then be evaluated in the next table `facemap_inference.FacemapPoseEstimation` according the the specifications of the `FacemapPoseEstimationTask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Facemap Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_model(model_name, model_description, full_model_path):\n",
    "    try:\n",
    "        model_id = max(facemap_inference.FacemapModel.fetch(\"model_id\"))\n",
    "        model_id = model_id + 1  # increment model id\n",
    "    except ValueError:  # case that nothing has been inserted\n",
    "        model_id = 0\n",
    "\n",
    "    facemap_model_insert = dict(\n",
    "        model_id=model_id, model_name=model_name, model_description=model_description\n",
    "    )\n",
    "    facemap_inference.FacemapModel.insert1(facemap_model_insert)\n",
    "\n",
    "    body_part_insert = []\n",
    "    body_parts = [\n",
    "        \"eye(back)\",\n",
    "        \"eye(bottom)\",\n",
    "        \"eye(front)\",\n",
    "        \"eye(top)\",\n",
    "        \"lowerlip\",\n",
    "        \"mouth\",\n",
    "        \"nose(bottom)\",\n",
    "        \"nose(r)\",\n",
    "        \"nose(tip)\",\n",
    "        \"nose(top)\",\n",
    "        \"nosebridge\",\n",
    "        \"paw\",\n",
    "        \"whisker(I)\",\n",
    "        \"whisker(III)\",\n",
    "        \"whisker(II)\",\n",
    "    ]\n",
    "    for bp in body_parts:\n",
    "        body_part_insert.append(dict(model_id=model_id, body_part=bp))\n",
    "    # Insert into parent BodyPart table if no entries are present\n",
    "    if len(facemap_inference.BodyPart()) == 0:\n",
    "        facemap_inference.BodyPart.insert(body_part_insert)\n",
    "    file_insert = dict(model_id=model_id, model_file=full_model_path)\n",
    "\n",
    "    facemap_inference.FacemapModel.BodyPart.insert(body_part_insert)\n",
    "    facemap_inference.FacemapModel.File.insert1(file_insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Subject and Session into subject.Subject, session.Session and session.SessionDirectory tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_insert = dict(subject=\"mdl_sub\", \n",
    "                subject_nickname=\"facemap model subject\", \n",
    "                sex='U', \n",
    "                subject_birth_date=datetime.datetime.now(), \n",
    "                subject_description=\"Subject for Facemap Model Inference testing\")\n",
    "# subject.Subject.insert1(sub_insert)\n",
    "subject_key = (subject.Subject & 'subject=\"mdl_sub\"').fetch1(\"KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = 2\n",
    "session_insert = dict(subject_key, session_id, session_datetime=datetime.datetime.now())\n",
    "sdir_insert = dict(subject_key, session_id, session_dir=\"20230627_Image_eCBsensor_activity/Behavior_20230627/C57-C11-3_Rm_CNO_30min\")\n",
    "\n",
    "session.Session.insert1(session_insert)\n",
    "session.SessionDirectory.insert1(sdir_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Session Table to validate insert\n",
    "session.Session() & {**subject_key, 'session_id': session_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display SessionDirectory Table to validate insert\n",
    "session.SessionDirectory() & {**subject_key, 'session_id': session_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest locally stored pytorch model(.pt) file\n",
    "Provide model name, model filepath, and optional model description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'facemap_model_state.pt'\n",
    "full_local_model_filepath = \"/Users/sidhulyalkar/.facemap/models/facemap_model_state.pt\"\n",
    "ingest_model(model_name, model_description=\"test facemap model\", model_file=full_local_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Pose Estimation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 0\n",
    "session_key = session.Session.fetch(\"KEY\")[2] \n",
    "generate_facemap_inference_estimation_task(model_id, session_key, task_mode=\"trigger\", bbox=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display FacemapPoseEstimationTask table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemap_inference.FacemapPoseEstimationTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display VideoRecording and VideoRecording.File tables from the imported facial behavioral estimation (fbe) schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbe.VideoRecording()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbe.VideoRecording.File()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Pose Estimation on all unprocessed FacemapPoseEstimationTasks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemap_inference.FacemapPoseEstimation.populate(display_progress=True)\n",
    "# If a lost connection error occurs, rerun the populate and if processing \n",
    "# has completed, the data will be loaded and inference will not be rerun. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Facemap Pose Estimation Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemap_inference.FacemapPoseEstimation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemap_inference.FacemapPoseEstimation.BodyPartPosition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Pose Estimation Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_query = {**session_key, 'recording_id': 0, 'model_id': model_id}\n",
    "pose_estimation_key = (facemap_inference.FacemapPoseEstimation & pe_query).fetch1(\"KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Trajectory of X and Y coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify all body parts, or set body_parts to a custom list\n",
    "body_parts = \"all\"\n",
    "model_name = (facemap_inference.FacemapModel & f'model_id={key[\"model_id\"]}').fetch1(\"model_name\")\n",
    "\n",
    "if body_parts == \"all\":\n",
    "    body_parts = (facemap_inference.BodyPartPosition & key).fetch(\"body_part\")\n",
    "elif not isinstance(body_parts, list):\n",
    "    body_parts = list(body_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Pandas MultiIndex DataFrame\n",
    "df = None\n",
    "for body_part in body_parts:\n",
    "    result_dict = (\n",
    "        facemap_inference.BodyPartPosition\n",
    "        & {\"body_part\": body_part}\n",
    "        & {\"recording_id\": key[\"recording_id\"]}\n",
    "        & {\"session_id\": key[\"session_id\"]}\n",
    "    ).fetch(\"x_pos\", \"y_pos\", \"likelihood\", as_dict=True)[0]\n",
    "    x_pos = result_dict[\"x_pos\"].tolist()\n",
    "    y_pos = result_dict[\"y_pos\"].tolist()\n",
    "    likelihood = result_dict[\"likelihood\"].tolist()\n",
    "    a = np.vstack((x_pos, y_pos, likelihood))\n",
    "    a = a.T\n",
    "    pdindex = pd.MultiIndex.from_product(\n",
    "        [[model_name], [body_part], [\"x\", \"y\", \"likelihood\"]],\n",
    "        names=[\"model\", \"bodyparts\", \"coords\"],\n",
    "    )\n",
    "    frame = pd.DataFrame(a, columns=pdindex, index=range(0, a.shape[0]))\n",
    "    df = pd.concat([df, frame], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or can use the built in function get_trajectory which also constructs this Pandas MultiIndex DataFrame\n",
    "# df=facemap_inference.FacemapPoseEstimation.get_trajectory(pose_estimation_key)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xy = df.iloc[:,df.columns.get_level_values(2).isin([\"x\",\"y\"])]['facemap_model_state.pt']\n",
    "df_xy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot coordinates across time for each body part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xy.plot().legend(loc='best', prop={'size': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = df_xy.copy()\n",
    "df_flat.columns = df_flat.columns.map('_'.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Trace Overlays of each body part across time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(2,2)\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(15)\n",
    "\n",
    "df_flat.plot(x='eye(front)_x',y='eye(front)_y',ax=ax[0, 0])\n",
    "df_flat.plot(x='eye(back)_x',y='eye(back)_y',ax=ax[0, 0])\n",
    "df_flat.plot(x='eye(bottom)_x',y='eye(bottom)_y',ax=ax[0, 0])\n",
    "\n",
    "df_flat.plot(x='nose(tip)_x',y='nose(tip)_y', ax=ax[1, 0])\n",
    "df_flat.plot(x='nose(bottom)_x',y='nose(bottom)_y', ax=ax[1, 0])\n",
    "df_flat.plot(x='nose(r)_x',y='nose(r)_y', ax=ax[1, 0])\n",
    "df_flat.plot(x='nosebridge_x',y='nosebridge_y', ax=ax[1, 0])\n",
    "\n",
    "df_flat.plot(x='mouth_x',y='mouth_y', ax=ax[0, 1])\n",
    "df_flat.plot(x='lowerlip_x',y='lowerlip_y', ax=ax[0, 1])\n",
    "df_flat.plot(x='paw_x',y='paw_y', ax=ax[0, 1])\n",
    "\n",
    "df_flat.plot(x='whisker(I)_x',y='whisker(I)_y', ax=ax[1, 1])\n",
    "df_flat.plot(x='whisker(II)_x',y='whisker(II)_y', ax=ax[1, 1])\n",
    "df_flat.plot(x='whisker(II)_x',y='whisker(II)_y', ax=ax[1, 1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
