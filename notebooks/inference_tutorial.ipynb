{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataJoint Elements for [Facemap](https://github.com/MouseLand/facemap) Pose Estimation\n",
    "\n",
    "#### Open-source data pipeline for orofacial movement tracking and pose estimation of distinct facial keypoints.\n",
    "\n",
    "Welcome to the tutorial for the DataJoint Element for pose estimation of distinct\n",
    "keypoints on the mouse face. This\n",
    "tutorial aims to provide a comprehensive understanding of the open-source data pipeline\n",
    "created using `element-facemap`'s pose estimation inference module.\n",
    "\n",
    "This package is designed to seamlessly process, ingest, and track facemap's pose\n",
    "estimation data, along with its associated video recording and model metadata. By the end of this\n",
    "tutorial you will have a clear grasp on setting up and integrating `element-facemap`\n",
    "into your specific research projects and lab. \n",
    "\n",
    "![flowchart](../images/diagram_flowchart.svg)\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Please see the [datajoint tutorials GitHub\n",
    "repository](https://github.com/datajoint/datajoint-tutorials/tree/main) before\n",
    "proceeding.\n",
    "\n",
    "A basic understanding of the following DataJoint concepts will be beneficial to your\n",
    "understanding of this tutorial: \n",
    "1. The `Imported` and `Computed` tables types in `datajoint-python`.\n",
    "2. The functionality of the `.populate()` method. \n",
    "\n",
    "#### **Tutorial Overview**\n",
    "\n",
    "+ Setup\n",
    "+ *Activate* the DataJoint pipeline.\n",
    "+ *Insert* subject and session metadata.\n",
    "+ *Populate* video recording and models metadata.\n",
    "+ Run the pose estimation task.\n",
    "+ Visualize the results.\n",
    "\n",
    "### **Setup**\n",
    "\n",
    "This tutorial examines video data of .mp4 file format. The goal is to store, track and\n",
    "manage sessions of facial pose data, including determining coordinates of facial body\n",
    "parts and trajectory visualizations.\n",
    "\n",
    "The results of this Element can be combined with **other modalities** to create a complete,\n",
    "customizable data pipeline for your specific lab or study. For instance, you can combine\n",
    "`element-facemap` with `element-calcium-imaging` and `element-array-ephys` to relate orofacial\n",
    "behavior to neural activity.\n",
    "\n",
    "Let's start this tutorial by importing the packages necessary to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the tutorial is run in Codespaces, a private, local database server is created and\n",
    "made available for you. This is where we will insert and store our processed results.\n",
    "Let's connect to the database server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.conn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Activate the DataJoint Pipeline**\n",
    "\n",
    "This tutorial activates the `facemap_inference.py` module from `element-facemap`, along\n",
    "with upstream dependencies from `element-animal` and `element-session`. Please refer to the\n",
    "[`tutorial_pipeline.py`](./tutorial_pipeline.py) for the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial_pipeline import lab, subject, session, fbe, facemap_inference, Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent the `fbe` (SVD-based facemap) and `facemap_inference` (pose-estimation\n",
    "on keypoints) schemas and their upstream dependencies, `session` and `subject` schemas\n",
    "as a diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    dj.Diagram(subject.Subject)\n",
    "    + dj.Diagram(session.Session)\n",
    "    + dj.Diagram(fbe.VideoRecording)\n",
    "    + dj.Diagram(fbe.VideoRecording.File)\n",
    "    + dj.Diagram(facemap_inference)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the diagram, this data pipeline encompasses tables associated with\n",
    "model and video file data, task generation and results of model inference. A few tables, such as `subject.Subject` or `session.Session`,\n",
    "while important for a complete pipeline, fall outside the scope of the `element-facemap`\n",
    "tutorial, and will therefore, not be explored extensively here. The primary focus of\n",
    "this tutorial will be on the `facemap_inference` schema.\n",
    "\n",
    "### **Insert subject, session, and probe metadata**\n",
    "\n",
    "Let's start with the first table in the schema diagram (i.e. `subject.Subject` table).\n",
    "\n",
    "To know what data to insert into the table, we can view its dependencies and attributes using the `.describe()` and `.heading` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subject.Subject.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells above show all attributes of the subject table.\n",
    "We will insert data into the\n",
    "`subject.Subject` table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.insert1(\n",
    "    dict(\n",
    "        subject=\"subject1\",\n",
    "        subject_nickname=\"subject1_nickname\",\n",
    "        sex=\"U\",\n",
    "        subject_birth_date=\"2020-01-01\",\n",
    "        subject_description=\"Demo subject for Facemap Pose estimation processing.\",\n",
    "    )\n",
    ")\n",
    "subject.Subject()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the steps above for the `Session` table and see how the output varies\n",
    "between `.describe` and `.heading`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.Session.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session.heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `describe`, displays the table's structure and highlights its dependencies, such as its reliance on the `Subject` table. These dependencies represent foreign key references, linking data across tables.\n",
    "\n",
    "On the other hand, `heading` provides an exhaustive list of the table's attributes. This\n",
    "list includes both the attributes declared in this table and any inherited from upstream\n",
    "tables.\n",
    "\n",
    "With this understanding, let's move on to insert a session associated with our subject.\n",
    "\n",
    "We will insert into the `session.Session` table by passing a dictionary to the `insert1` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_key = dict(subject=\"subject1\", session_datetime=\"2021-04-30 12:22:15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session.insert1(session_key)\n",
    "session.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every experimental session produces a set of data files. The purpose of the `SessionDirectory` table is to locate these files. It references a directory path relative to a root directory, defined in `dj.config[\"custom\"]`. More information about `dj.config` is provided in the [documentation](https://datajoint.com/docs/elements/user-guide/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.SessionDirectory.insert1(dict(**session_key, session_dir=\"subject1/session1\"))\n",
    "session.SessionDirectory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Diagram indicates, both `fbe.VideoRecording` and `facemap_inference.FacemapModel` tables require entries before other downstream tables can receive data. \n",
    "\n",
    "Let's start by inserting a local pytorch model file into the `facemap_inference.FacemapModel` table. This table requires the following:\n",
    "- Specify a unique `model_id`, a `model_description`, and the `full_local_model_filepath`\n",
    "\n",
    "Note: *when running locally, the default facemap model is located in the hidden `.facemap` folder installed to your computer's home directory: `i.e. ~/.facemap/models/facemap_model_state.pt`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facemap_model_state.pt\"\n",
    "model_id = 0\n",
    "model_description = \"test facemap model\"\n",
    "full_local_model_filepath = \"../example_data/inbox/facemap_models/default_facemap_model.pt\"\n",
    "facemap_inference.FacemapModel.insert_new_model(\n",
    "    model_name=model_name,\n",
    "    model_id=model_id,\n",
    "    model_description=model_description,\n",
    "    full_model_path=full_local_model_filepath,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the `facemap_inference.FacemapModel` table to verify insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemap_inference.FacemapModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's display the `facemap_inference.FacemapModel.File` and `facemap_inference.FacemapModel.BodyPart` part tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemap_inference.FacemapModel.File()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemap_inference.FacemapModel.BodyPart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next table that needs data is `fbe.VideoRecording` before the\n",
    "a pose estimation task can be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_recording_key = {**session_key, \"recording_id\": 0}\n",
    "facemap_root_dir_path = fbe.get_facemap_root_data_dir()\n",
    "vid_path = \"./example_data/inbox/subject0/session0/*.avi\"\n",
    "video_recording_file_insert = {\n",
    "    **video_recording_key,\n",
    "    \"file_id\": 0,\n",
    "    \"file_path\": Path(vid_path).relative_to(facemap_root_dir_path[0]),\n",
    "}\n",
    "\n",
    "fbe.VideoRecording.insert1(video_recording_key)\n",
    "fbe.VideoRecording.File.insert1(video_recording_file_insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an entries present in the `facemap_inference.FacemapModel` and the\n",
    "`fbe.VideoRecording` tables, the criteria for insertion into the\n",
    "`facemap_inference.FacemapPoseEstimationTask` table is met.\n",
    "\n",
    "- `facemap_inference.FacemapPoseEstimationTask` is a staging table that pairs a specific `FacemapModel` with a `VideoRecording`.\n",
    "- For this example, we will choose to `load` existing results due to the speed of processing. \n",
    "- If choosing to run processing, the `task_mode` should be set to `trigger`. This step may take some time and can result in a lost connection database issue, to solve this simply rerun the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Pose estimation is then be evaluated in the next table `facemap_inference.FacemapPoseEstimation` according the the specifications of the `FacemapPoseEstimationTask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key = (facemap_inference.FacemapModel & f\"model_id={model_id}\").fetch1(\"KEY\")\n",
    "key = {**video_recording_key, **model_key}\n",
    "task_description = \"Demo Facemap Inference Task, loads processed results\"\n",
    "facemap_inference.FacemapPoseEstimationTask.insert_pose_estimation_task(\n",
    "    key, task_description=task_description, task_mode=\"load\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the key in the `FacemapPoseEstimationTask` table to confirm it was inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(facemap_inference.FacemapPoseEstimationTask() & key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will ingest the results into `FacemapPoseEstimation` and its part table `FacemapPoseEstimation.BodyPartPosition` for the key that we just inserted into the `FacemapPoseEstimationTask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemap_inference.FacemapPoseEstimation.populate(key, display_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the cell above has completed run the next cells to display the `FacemapPoseEstimation` tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemap_inference.FacemapPoseEstimation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemap_inference.FacemapPoseEstimation.BodyPartPosition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualize the results**\n",
    "In this tutorial, we will do some exploratory analysis by fetching the data from the database and creating a few plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_query = {**session_key, \"recording_id\": 0, \"model_id\": model_id}\n",
    "pose_estimation_key = (facemap_inference.FacemapPoseEstimation & pe_query).fetch1(\"KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Trajectory of X and Y coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify all body parts, or set body_parts to a custom list\n",
    "body_parts = \"all\"\n",
    "model_name = (facemap_inference.FacemapModel & f'model_id={key[\"model_id\"]}').fetch1(\n",
    "    \"model_name\"\n",
    ")\n",
    "\n",
    "if body_parts == \"all\":\n",
    "    body_parts = (facemap_inference.FacemapPoseEstimation.BodyPartPosition & key).fetch(\n",
    "        \"body_part\"\n",
    "    )\n",
    "elif not isinstance(body_parts, list):\n",
    "    body_parts = list(body_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Pandas MultiIndex DataFrame\n",
    "df = None\n",
    "for body_part in body_parts:\n",
    "    result_dict = (\n",
    "        facemap_inference.FacemapPoseEstimation.BodyPartPosition\n",
    "        & {\"body_part\": body_part}\n",
    "        & {\"recording_id\": key[\"recording_id\"]}\n",
    "        & {\"session_id\": key[\"session_id\"]}\n",
    "    ).fetch(\"x_pos\", \"y_pos\", \"likelihood\", as_dict=True)[0]\n",
    "    x_pos = result_dict[\"x_pos\"].tolist()\n",
    "    y_pos = result_dict[\"y_pos\"].tolist()\n",
    "    likelihood = result_dict[\"likelihood\"].tolist()\n",
    "    a = np.vstack((x_pos, y_pos, likelihood))\n",
    "    a = a.T\n",
    "    pdindex = pd.MultiIndex.from_product(\n",
    "        [[model_name], [body_part], [\"x\", \"y\", \"likelihood\"]],\n",
    "        names=[\"model\", \"bodyparts\", \"coords\"],\n",
    "    )\n",
    "    frame = pd.DataFrame(a, columns=pdindex, index=range(0, a.shape[0]))\n",
    "    df = pd.concat([df, frame], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xy = df.iloc[:, df.columns.get_level_values(2).isin([\"x\", \"y\"])][\n",
    "    \"facemap_model_state.pt\"\n",
    "]\n",
    "df_xy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot coordinates across time for each body part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xy.plot().legend(loc=\"best\", prop={\"size\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = df_xy.copy()\n",
    "df_flat.columns = df_flat.columns.map(\"_\".join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Trace Overlays of each body part across time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2)\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(15)\n",
    "\n",
    "df_flat.plot(x=\"eye(front)_x\", y=\"eye(front)_y\", ax=ax[0, 0])\n",
    "df_flat.plot(x=\"eye(back)_x\", y=\"eye(back)_y\", ax=ax[0, 0])\n",
    "df_flat.plot(x=\"eye(bottom)_x\", y=\"eye(bottom)_y\", ax=ax[0, 0])\n",
    "\n",
    "df_flat.plot(x=\"nose(tip)_x\", y=\"nose(tip)_y\", ax=ax[1, 0])\n",
    "df_flat.plot(x=\"nose(bottom)_x\", y=\"nose(bottom)_y\", ax=ax[1, 0])\n",
    "df_flat.plot(x=\"nose(r)_x\", y=\"nose(r)_y\", ax=ax[1, 0])\n",
    "df_flat.plot(x=\"nosebridge_x\", y=\"nosebridge_y\", ax=ax[1, 0])\n",
    "\n",
    "df_flat.plot(x=\"mouth_x\", y=\"mouth_y\", ax=ax[0, 1])\n",
    "df_flat.plot(x=\"lowerlip_x\", y=\"lowerlip_y\", ax=ax[0, 1])\n",
    "df_flat.plot(x=\"paw_x\", y=\"paw_y\", ax=ax[0, 1])\n",
    "\n",
    "df_flat.plot(x=\"whisker(I)_x\", y=\"whisker(I)_y\", ax=ax[1, 1])\n",
    "df_flat.plot(x=\"whisker(II)_x\", y=\"whisker(II)_y\", ax=ax[1, 1])\n",
    "df_flat.plot(x=\"whisker(II)_x\", y=\"whisker(II)_y\", ax=ax[1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualize Keypoints Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "colors = cm.get_cmap(\"jet\")(np.linspace(0, 1.0, len(body_parts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the x and y coordinates stored in the `BodyPartPosition` table, for the `pose_estimation_key` of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(facemap_inference.FacemapPoseEstimation.BodyPartPosition & pose_estimation_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the keypoints_data from the database as a dictionary in order to index and reshape it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_data = (\n",
    "    facemap_inference.FacemapPoseEstimation.BodyPartPosition & pose_estimation_key\n",
    ").fetch(as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_x_coord = []\n",
    "pose_y_coord = []\n",
    "pose_likelihood = []\n",
    "for body_part_data in keypoints_data:\n",
    "    pose_x_coord.append(body_part_data[\"x_pos\"][:])\n",
    "    pose_y_coord.append(body_part_data[\"y_pos\"][:])\n",
    "    pose_likelihood.append(body_part_data[\"likelihood\"][:])\n",
    "\n",
    "pose_x_coord = np.array([pose_x_coord])  # size: key points x frames\n",
    "pose_y_coord = np.array([pose_y_coord])  # size: key points x frames\n",
    "pose_likelihood = np.array([pose_likelihood])  # size: key points x frames\n",
    "pose_data = np.concatenate(\n",
    "    (pose_x_coord, pose_y_coord, pose_likelihood), axis=0\n",
    ")  # size: 3 x key points x frames\n",
    "pose_x_coord = pose_data[0, :, :]\n",
    "pose_y_coord = pose_data[1, :, :]\n",
    "pose_liklihood = pose_data[2, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot keypoints for a subset of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_frame = 100\n",
    "end_frame = 500\n",
    "\n",
    "plt.figure(figsize=(15, 5), dpi=100)\n",
    "for i, bodypart in enumerate(body_parts):\n",
    "    plt.plot(\n",
    "        np.arange(start_frame, end_frame),\n",
    "        pose_x_coord[i, start_frame:end_frame],\n",
    "        \"-\",\n",
    "        c=colors[i],\n",
    "        label=bodypart,\n",
    "    )\n",
    "    plt.plot(\n",
    "        np.arange(start_frame, end_frame),\n",
    "        pose_y_coord[i, start_frame:end_frame],\n",
    "        \"--\",\n",
    "        c=colors[i],\n",
    "    )\n",
    "plt.xlabel(\"Frame\")\n",
    "plt.ylabel(\"Keypoint coordinates\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a subset of bodypart keypoints for a subset of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_bodyparts = [\"whisker(I)\", \"whisker(II)\", \"whisker(III)\"]\n",
    "start_frame = 100\n",
    "end_frame = 500\n",
    "\n",
    "plt.figure(figsize=(15, 5), dpi=100)\n",
    "for i, bodypart in enumerate(body_parts):\n",
    "    if bodypart in subset_bodyparts:\n",
    "        plt.plot(\n",
    "            np.arange(start_frame, end_frame),\n",
    "            pose_x_coord[i, start_frame:end_frame],\n",
    "            \"-\",\n",
    "            c=colors[i],\n",
    "            label=bodypart,\n",
    "        )\n",
    "        plt.plot(\n",
    "            np.arange(start_frame, end_frame),\n",
    "            pose_y_coord[i, start_frame:end_frame],\n",
    "            \"--\",\n",
    "            c=colors[i],\n",
    "        )\n",
    "plt.xlabel(\"Frame\")\n",
    "plt.ylabel(\"Keypoint coordinates\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter keypoints data by confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Facemap's `filter_outliers` function to remove outliers by applying a median filter to the keypoints data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facemap.utils import filter_outliers\n",
    "\n",
    "# Use the following function to filter outliers in the keypoints data (see docstring for details)\n",
    "\"\"\"\n",
    "utils.filter_outliers(x, y, filter_window=15, baseline_window=50, max_spike=25, max_diff=25)\n",
    "x: x coordinates of keypoints\n",
    "y: y coordinates of keypoints\n",
    "filter_window: window size for median filter (default: 15)\n",
    "baseline_window: window size for baseline estimation (default: 50)\n",
    "max_spike: maximum spike size (default: 25)\n",
    "max_diff: maximum difference between baseline and filtered signal (default: 25)\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(15, 5), dpi=100)\n",
    "for i, bodypart in enumerate(body_parts):\n",
    "    if bodypart in subset_bodyparts:\n",
    "        x, y = filter_outliers(pose_x_coord[i], pose_y_coord[i])\n",
    "        plt.plot(\n",
    "            np.arange(start_frame, end_frame),\n",
    "            x[start_frame:end_frame],\n",
    "            \"-\",\n",
    "            c=colors[i],\n",
    "            label=bodypart,\n",
    "        )\n",
    "        plt.plot(\n",
    "            np.arange(start_frame, end_frame),\n",
    "            y[start_frame:end_frame],\n",
    "            \"--\",\n",
    "            c=colors[i],\n",
    "        )\n",
    "plt.xlabel(\"Frame\")\n",
    "plt.ylabel(\"Keypoint coordinates\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\n",
    "plt.title(\"Filtered keypoints\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
